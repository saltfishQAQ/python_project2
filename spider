# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import datetime
import re
import pandas as pd
import time
start = time.process_time()
def getnews2(starttime,endtime,keywords):
    titlelis= []
    urllis = []
    dates = []
    urllis2 = []
    titles = []
    years = []
    months = []
    days = []
    dt = datetime.datetime.strptime(starttime, "%Y%m%d")
    date = starttime[:]
    while date <= endtime:
        dates.append(date)
        dt = dt + datetime.timedelta(1)
        date = dt.strftime("%Y%m%d")
    for a in dates:
        year = a[0:4]
        month = a[4:6]
        day = a[6:8]
        res = requests.get('http://www.macaodaily.com/html/{0}-{1}/{2}/node_1.htm'.format(year,month,day))
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text,"lxml")
        Index1 = soup.find_all("li")
        for b in Index1:
            name = b.get_text()
            url = "http://www.macaodaily.com/html/{0}-{1}/{2}/".format(year,month,day)+b.find("a")["href"]
            titlelis.append(name)
            urllis.append(url)
    for each_url,c in zip(urllis,range(len(urllis))):
        print("正在分析网址,",each_url)
        print("分析到第{0}个，共{1}".format(c+1, len(urllis)))
        article = requests.get(each_url)
        article.encoding = 'utf-8'
        soup_article = BeautifulSoup(article.text, "lxml")
        Index2 = soup_article.find("div",{"id": "ozoom"}).get_text()
        for d in keywords:
            if d in Index2:
                print(each_url,"有关键词",d)
                urllis2.append(each_url)
                titles.append(titlelis[c])
                time = re.search("http://www.macaodaily.com/html/(.*?)-(.*?)/(.*?)/",each_url,re.S)
                years.append(time.group(1))
                months.append(time.group(2))
                days.append(time.group(3))
            else:
                continue
    news = {'year': years, 'month': months,
               'day': days, 'title': titles, 'url': urllis2}
    news1 = pd.DataFrame(news)
    news2 = news1.drop_duplicates(subset='url', keep='first',ignore_index=True)
    news2.to_csv('trump.csv', sep=',', encoding='utf-8-sig')
    print(news)
starttime = input("请输入开始时间: (如 20180201)")
endtime = input("请输入结束时间: (如 20180210)")
keywords = eval(input("请输入关键词: (如 ['澳門','珠海','北京'])"))
getnews2(starttime,endtime,keywords)
elapsed = (time.process_time() - start)
print(elapsed)
"""
20201001
20201123
['特朗普','邦登','川普','美國大選','總統']
"""
